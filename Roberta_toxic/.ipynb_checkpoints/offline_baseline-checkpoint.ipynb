{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ccd4c79",
   "metadata": {},
   "source": [
    "Загружаем скачанный классификатор токсичности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffaceb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "fname = 'data.tgz'\n",
    "\n",
    "if fname.endswith(\"tgz\"):\n",
    "    tar = tarfile.open(fname, \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "elif fname.endswith(\"tar\"):\n",
    "    tar = tarfile.open(fname, \"r:\")\n",
    "    tar.extractall()\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eccdfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"trained_roberta/\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"trained_roberta/\")\n",
    "try:\n",
    "    model = model.cuda()\n",
    "except:\n",
    "    model = model\n",
    "\n",
    "TOXIC_CLASS=-1\n",
    "TOKENIZATION_TYPE='sentencepiece'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c1e32",
   "metadata": {},
   "source": [
    "Ниже функции для применения классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3c6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import softmax, sigmoid\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ALLOWED_ALPHABET=list(map(chr, range(ord('а'), ord('я') + 1)))\n",
    "ALLOWED_ALPHABET.extend(map(chr, range(ord('a'), ord('z') + 1)))\n",
    "ALLOWED_ALPHABET.extend(list(map(str.upper, ALLOWED_ALPHABET)))\n",
    "ALLOWED_ALPHABET = set(ALLOWED_ALPHABET)\n",
    "\n",
    "\n",
    "def logits_to_toxic_probas(logits):\n",
    "    if logits.shape[-1] > 1:\n",
    "        activation = lambda x: softmax(x, -1)\n",
    "    else:\n",
    "        activation = sigmoid\n",
    "    return activation(logits)[:, TOXIC_CLASS].cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def is_word_start(token):\n",
    "    if TOKENIZATION_TYPE == 'sentencepiece':\n",
    "        return token.startswith('▁')\n",
    "    if TOKENIZATION_TYPE == 'bert':\n",
    "        return not token.startswith('##')\n",
    "    raise ValueError(\"Unknown tokenization type\")\n",
    "\n",
    "\n",
    "def normalize(sentence, max_tokens_per_word=20):\n",
    "    sentence = ''.join(map(lambda c: c if c.isalpha() else ' ', sentence.lower()))\n",
    "    ids = tokenizer(sentence)['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)[1:-1]\n",
    "    \n",
    "    result = []\n",
    "    num_continuation_tokens = 0\n",
    "    for token in tokens:\n",
    "        if not is_word_start(token):\n",
    "            num_continuation_tokens += 1\n",
    "            if num_continuation_tokens < max_tokens_per_word:\n",
    "                result.append(token.lstrip('#▁'))\n",
    "        else:\n",
    "            num_continuation_tokens = 0\n",
    "            result.extend([' ', token.lstrip('▁#')])\n",
    "    \n",
    "    return ''.join(result).strip()\n",
    "\n",
    "def iterate_batches(data, batch_size=40):\n",
    "    batch = []\n",
    "    for x in data:\n",
    "        batch.append(x)\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "def predict_toxicity(sentences, batch_size=5, threshold=0.5, return_scores=False, verbose=True, device='cuda'):\n",
    "    results = []\n",
    "    tqdm_fn = tqdm if verbose else lambda x, total: x\n",
    "    for batch in tqdm_fn(iterate_batches(sentences, batch_size), total=np.ceil(len(sentences) / batch_size)):\n",
    "        normlized = [normalize(sent, max_tokens_per_word=5) for sent in batch]\n",
    "        tokenized = tokenizer(normlized, return_tensors='pt', padding=True, max_length=512, truncation=True)\n",
    "        \n",
    "        logits = model.to(device)(**{key: val.to(device) for key, val in tokenized.items()}).logits\n",
    "        preds = logits_to_toxic_probas(logits)\n",
    "        if not return_scores:\n",
    "            preds = preds >= threshold\n",
    "        results.extend(preds)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d338798e",
   "metadata": {},
   "source": [
    "Читаем тестовый набор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "351d6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "with open('dev_data.csv', 'rt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        texts.append(normalize(line)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711eba2",
   "metadata": {},
   "source": [
    "Вычисляем токсичность отдельных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76446780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa585eda02b4d649e4486f4bdf3ae0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1852f5242547>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mword_toxicities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_toxicity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtoxicity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_toxicities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-cda85d450e22>\u001b[0m in \u001b[0;36mpredict_toxicity\u001b[1;34m(sentences, batch_size, threshold, return_scores, verbose, device)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormlized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits_to_toxic_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mreturn_scores\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "words = set()\n",
    "for text in texts:\n",
    "    words.update(text.split())\n",
    "words = sorted(words)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    word_toxicities = predict_toxicity(words, batch_size=100, return_scores=True)\n",
    "    \n",
    "toxicity = dict(zip(words, word_toxicities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d3d91b",
   "metadata": {},
   "source": [
    "Ниже читаем эмбеддинги слов и описываем функции их обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01bc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "stemmer = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "217a29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_file = np.load('embeddings_with_lemmas.npz', allow_pickle=True)\n",
    "embs_vectors = embs_file['vectors']\n",
    "embs_vectors_normed = embs_vectors / np.linalg.norm(embs_vectors, axis=1, keepdims=True)\n",
    "embs_voc = embs_file['voc'].item()\n",
    "\n",
    "embs_voc_by_id = [None for i in range(len(embs_vectors))]\n",
    "for word, idx in embs_voc.items():\n",
    "    if embs_voc_by_id[idx] is None:\n",
    "        embs_voc_by_id[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fbb7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_indicies(a):\n",
    "    res = []\n",
    "    if isinstance(a, str):\n",
    "        a = a.split()\n",
    "    for w in a:\n",
    "        if w in embs_voc:\n",
    "            res.append(embs_voc[w])\n",
    "        else:\n",
    "            lemma = stemmer.lemmatize(w)[0]\n",
    "            res.append(embs_voc.get(lemma, None))\n",
    "    return res\n",
    "\n",
    "def calc_embs(words):\n",
    "    words = ' '.join(map(normalize, words))\n",
    "    inds = get_w2v_indicies(words)\n",
    "    return [None if i is None else embs_vectors[i] for i in inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724c2319",
   "metadata": {},
   "source": [
    "Сложим эмбеддинги нетоксичных слов в kd-дерево, чтобы можно было близко искать ближайших соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e2934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nontoxic_emb_inds = [ind for word, ind in embs_voc.items() if toxicity.get(word, 1.0) <= 0.5]\n",
    "embs_vectors_normed_nontoxic = embs_vectors_normed[nontoxic_emb_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6012276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "embs_tree = KDTree(embs_vectors_normed_nontoxic, leaf_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6fa02",
   "metadata": {},
   "source": [
    "Функция находит самое близкое нетоксичное слово по предпосчитанным эмбеддингам слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d97c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache()\n",
    "def find_closest_nontoxic(word, threshold=0.5, allow_self=False):\n",
    "    if toxicity.get(word, 1.0) <= threshold:\n",
    "        return word\n",
    "    \n",
    "    if word not in toxicity and word not in embs_voc:\n",
    "        return None\n",
    "    \n",
    "    threshold = min(toxicity.get(word, threshold), threshold)\n",
    "    word = normalize(word)\n",
    "    word_emb = calc_embs([word])\n",
    "    if word_emb is None or word_emb[0] is None:\n",
    "        return None\n",
    "    \n",
    "    for i in embs_tree.query(word_emb)[1][0]:\n",
    "        other_word = embs_voc_by_id[nontoxic_emb_inds[i]]\n",
    "        if (other_word != word or allow_self) and toxicity.get(other_word, 1.0) <= threshold:\n",
    "            return other_word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ee566",
   "metadata": {},
   "source": [
    "Заменяем токсичные слова на ближайшие по эмбеддингам не-токсичные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5098156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detox(line):\n",
    "    words = normalize(line).split()\n",
    "    fixed_words = [find_closest_nontoxic(word, allow_self=True) or '' for word in words]\n",
    "    return ' '.join(fixed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f2cebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f2386b32d2445c8007a8acf23f0a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fixed_texts = list(map(detox, tqdm(texts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce5006",
   "metadata": {},
   "source": [
    "запишем результат в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b031cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baseline_fixed.txt', 'wt') as f:\n",
    "    for text in fixed_texts:\n",
    "        print(text, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22035319",
   "metadata": {},
   "source": [
    "Скор, если никак не изменять комментарии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "947f5909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dimdi-y/.local/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (2.3.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Loading tokenizer\n",
      "Loading model\n",
      "Loading texts\n",
      "Loading LM\n",
      "Loading embeddings\n",
      "Scoring\n",
      " 10%|████                                    | 50/500.0 [00:01<00:15, 29.21it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████| 500/500.0 [00:20<00:00, 24.28it/s]\n",
      "2500it [00:26, 95.03it/s] \n",
      "average toxicity: 0.6330938\n",
      "mean lmdiff: 1.0\n",
      "mean distance_score: 1.0\n",
      "36.69\n"
     ]
    }
   ],
   "source": [
    "!python3.7 score.py public_testset.short.txt public_testset.short.txt  --embeddings embeddings_with_lemmas.npz --lm lm.binary --model ./trained_roberta/ --device cuda --score -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ed959",
   "metadata": {},
   "source": [
    "Скор бейзлайна:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feaeedef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dimdi-y/.local/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (2.3.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Loading tokenizer\n",
      "Loading model\n",
      "Loading texts\n",
      "Loading LM\n",
      "Loading embeddings\n",
      "Scoring\n",
      " 20%|███████▊                               | 100/500.0 [00:03<00:14, 27.69it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████| 500/500.0 [00:19<00:00, 25.01it/s]\n",
      "2500it [00:40, 62.24it/s]\n",
      "average toxicity: 0.46444112\n",
      "mean lmdiff: 0.9444674231112382\n",
      "mean distance_score: 0.8119417961430562\n",
      "42.11\n"
     ]
    }
   ],
   "source": [
    "!python3.7 score.py public_testset.short.txt baseline_fixed.txt  --embeddings embeddings_with_lemmas.npz --lm lm.binary --model ./trained_roberta/ --device cuda --score -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15418efa",
   "metadata": {},
   "source": [
    "Сохраним данные для бейзлайна online-задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f7e462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p online_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4475e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('./online_baseline/data.pkl', 'wb') as f:\n",
    "    pkl.dump(toxicity, f)\n",
    "    pkl.dump(nontoxic_emb_inds, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
